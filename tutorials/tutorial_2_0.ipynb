{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNk7IylTv610"
      },
      "source": [
        "# Loading and Analysing Pre-Trained Sparse Autoencoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_DusoOvwV0M"
      },
      "source": [
        "## Set Up (Just Run / Not Important)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfDUxRx0wSRl"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import google.colab # type: ignore\n",
        "    from google.colab import output\n",
        "    COLAB = True\n",
        "    %pip install sae-lens transformer-lens sae-dashboard\n",
        "except:\n",
        "    COLAB = False\n",
        "    from IPython import get_ipython # type: ignore\n",
        "    ipython = get_ipython(); assert ipython is not None\n",
        "    ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
        "    ipython.run_line_magic(\"autoreload\", \"2\")\n",
        "\n",
        "# Standard imports\n",
        "import os\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "from pprint import pp as print\n",
        "\n",
        "# Imports for displaying vis in Colab / notebook\n",
        "\n",
        "torch.set_grad_enabled(False);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQSD7trbv610",
        "outputId": "222a40c4-75d4-46e2-ed3f-991841144926"
      },
      "outputs": [],
      "source": [
        "# For the most part I'll try to import functions and classes near where they are used\n",
        "# to make it clear where they come from.\n",
        "\n",
        "if torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "else:\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(f\"Device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoMx3VZpv611"
      },
      "source": [
        "# Loading a pretrained Sparse Autoencoder\n",
        "\n",
        "Below we load a Transformerlens model, a pretrained SAE and a dataset from huggingface. To see which SAEs are available, we can use the following function from SAE Lens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sae_lens.toolkit.pretrained_saes_directory import get_pretrained_saes_directory\n",
        "\n",
        "# TODO: Make this nicer.\n",
        "df = pd.DataFrame.from_records({k:v.__dict__ for k,v in get_pretrained_saes_directory().items()})\n",
        "df.T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNSfL80Uv611"
      },
      "outputs": [],
      "source": [
        "# from transformer_lens import HookedTransformer\n",
        "from sae_lens import SAE, HookedSAETransformer\n",
        "\n",
        "model = HookedSAETransformer.from_pretrained(\"gpt2-small\", device = device)\n",
        "\n",
        "# the cfg dict is returned alongside the SAE since it may contain useful information for analysing the SAE (eg: instantiating an activation store)\n",
        "# Note that this is not the same as the SAEs config dict, rather it is whatever was in the HF repo, from which we can extract the SAE config dict\n",
        "# We also return the feature sparsities which are stored in HF for convenience. \n",
        "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
        "    release = \"gpt2-small-res-jb\", # see other options in sae_lens/pretrained_saes.yaml\n",
        "    sae_id = \"blocks.7.hook_resid_pre\", # won't always be a hook point\n",
        "    device = device\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The \"sae\" object is an instance of the SAE (Sparse Autoencoder class). There are many different SAE architectures which may have different weights or activation functions. In order to simplify working with SAEs, SAE Lens handles most of this complexity for you. \n",
        "\n",
        "Let's look at the SAE config and understand each of the parameters:\n",
        "1. `architecture`\n",
        "2. `d_in`\n",
        "\n",
        "TODO: Finish writing this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(sae.cfg.__dict__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next we need to load in a dataset to work with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset  \n",
        "from transformer_lens.utils import tokenize_and_concatenate\n",
        "\n",
        "dataset = load_dataset(\n",
        "    path = \"NeelNanda/pile-10k\",\n",
        "    split=\"train\",\n",
        "    streaming=False,\n",
        ")\n",
        "\n",
        "token_dataset = tokenize_and_concatenate(\n",
        "    dataset= dataset,# type: ignore\n",
        "    tokenizer = model.tokenizer, # type: ignore\n",
        "    streaming=True,\n",
        "    max_length=sae.cfg.context_size,\n",
        "    add_bos_token=sae.cfg.prepend_bos,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Basics: What are SAE Features?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Opening a feature dashboard on Neuronpedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import IFrame\n",
        "\n",
        "# get a random feature from the SAE\n",
        "feature_idx = torch.randint(0, sae.cfg.d_sae, (1,)).item()\n",
        "\n",
        "html_template = \"https://neuronpedia.org/{}/{}/{}?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n",
        "\n",
        "def get_html(sae_release = \"gpt2-small\", sae_id=\"7-res-jb\", feature_idx=0):\n",
        "    return html_template.format(sae_release, sae_id, feature_idx)\n",
        "\n",
        "html = get_html(sae_release = \"gpt2-small\", sae_id=\"7-res-jb\", feature_idx=feature_idx)\n",
        "IFrame(html, width=1200, height=600)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What if we wanted to search for a feature relating to a specific thing? Then we could use the explanation search api. Let's just download all the explanations for these SAE features and load them in as a pandas dataframe. The Neuronpedia API docs will be useful here: https://www.neuronpedia.org/api-doc#tag/explanations/POST/api/explanation/export."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://www.neuronpedia.org/api/explanation/export\"\n",
        "\n",
        "payload = {\n",
        "    \"modelId\": \"gpt2-small\",\n",
        "    \"saeId\": \"7-res-jb\",\n",
        "}\n",
        "headers = {\"Content-Type\": \"application/json\"}\n",
        "\n",
        "response = requests.post(url, json=payload, headers=headers)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# convert to pandas\n",
        "explanations_df = pd.DataFrame(response.json()[\"explanations\"])\n",
        "# rename index to \"feature\"\n",
        "explanations_df.rename(columns={\"index\": \"feature\"}, inplace=True)\n",
        "# explanations_df[\"feature\"] = explanations_df[\"feature\"].astype(int)\n",
        "explanations_df[\"description\"] = explanations_df[\"description\"].apply(lambda x: x.lower())\n",
        "explanations_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's search for a feature to do with the bible. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bible_features = explanations_df.loc[explanations_df.description.str.contains(\" bible\")]\n",
        "bible_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's get the\n",
        "html = get_html(sae_release = \"gpt2-small\", sae_id=\"7-res-jb\", feature_idx=bible_features.feature.values[0])\n",
        "IFrame(html, width=1200, height=600)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Basics: Getting Features Using SAEs\n",
        "\n",
        "Let's start by finding the features which fire on a specific prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformer_lens.utils import test_prompt\n",
        "\n",
        "prompt = \"In the beginning, God created the heavens and the\"\n",
        "answer = \"earth\"\n",
        "\n",
        "test_prompt(prompt, answer, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Method 1: Using HookedSAETransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sae.use_error_term # If use error term is set to false, we will modify the forward pass by using the sae."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# hooked SAE Transformer will enable us to get the feature activations from the SAE\n",
        "_, cache = model.run_with_cache_with_saes(prompt, saes=[sae])\n",
        "\n",
        "print([(k, v.shape) for k,v in cache.items() if \"sae\" in k])\n",
        "\n",
        "# note there were 11 tokens in our prompt, the residual stream dimension is 768, and the number of SAE features is 768"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# let's look at which features fired at layer 8 at the final token position\n",
        "px.line(\n",
        "    cache['blocks.7.hook_resid_pre.hook_sae_acts_post'][0, -1, :].cpu().numpy(),\n",
        "    title=\"Feature activations at the final token position\",\n",
        "    labels={\"index\": \"Feature\", \"value\": \"Activation\"},\n",
        ")\n",
        "\n",
        "# let's print the top 5 features and how much they fired\n",
        "vals, inds = torch.topk(cache['blocks.7.hook_resid_pre.hook_sae_acts_post'][0, -1, :], 5)\n",
        "\n",
        "for val, ind in zip(vals, inds):\n",
        "    print(f\"Feature {ind} fired {val:.2f}\")\n",
        "    html = get_html(sae_release = \"gpt2-small\", sae_id=\"7-res-jb\", feature_idx=ind)\n",
        "    display(IFrame(html, width=1200, height=300))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Contrast Pairs Trick\n",
        "\n",
        "Sometimes we may be interested in which features fire differently between two prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformer_lens.utils import test_prompt\n",
        "\n",
        "prompt = \"In the beginning, God created the cat and the\"\n",
        "answer = \"earth\"\n",
        "\n",
        "test_prompt(prompt, answer, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = [\"In the beginning, God created the heavens and the\", \"In the beginning, God created the cat and the\"]\n",
        "_, cache = model.run_with_cache_with_saes(prompt, saes=[sae])\n",
        "print([(k, v.shape) for k,v in cache.items() if \"sae\" in k])\n",
        "\n",
        "feature_activation_df = pd.DataFrame(cache['blocks.7.hook_resid_pre.hook_sae_acts_post'][0, -1, :].cpu().numpy(),\n",
        "                                     index = [f\"feature_{i}\" for i in range(sae.cfg.d_sae)],\n",
        ")\n",
        "feature_activation_df.columns = [\"heavens_and_the\"]\n",
        "feature_activation_df[\"cat_and_the\"] = cache['blocks.7.hook_resid_pre.hook_sae_acts_post'][1, -1, :].cpu().numpy()\n",
        "feature_activation_df[\"diff\"]= feature_activation_df[\"heavens_and_the\"] - feature_activation_df[\"cat_and_the\"]\n",
        "\n",
        "fig = px.line(\n",
        "    feature_activation_df,\n",
        "    title=\"Feature activations for the prompt\",\n",
        "    labels={\"index\": \"Feature\", \"value\": \"Activation\"},\n",
        ")\n",
        "\n",
        "# hide the x-ticks\n",
        "fig.update_xaxes(showticklabels=False)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# let's look at the biggest features in terms of absolute difference\n",
        "\n",
        "diff = cache['blocks.7.hook_resid_pre.hook_sae_acts_post'][1, -1, :].cpu() - cache['blocks.7.hook_resid_pre.hook_sae_acts_post'][0, -1, :].cpu()\n",
        "vals, inds = torch.topk(torch.abs(diff), 5)\n",
        "for val, ind in zip(vals, inds):\n",
        "    print(f\"Feature {ind} had a difference of {val:.2f}\")\n",
        "    html = get_html(sae_release = \"gpt2-small\", sae_id=\"7-res-jb\", feature_idx=ind)\n",
        "    display(IFrame(html, width=1200, height=300))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Making Feature Dashboards\n",
        "\n",
        "Basic feature dashboards have 4 main components:\n",
        "1. Feature Activation Distribution. We report the proportion of tokens a feature fires on, usually between 1 in every 100 and 1 in every 10,000 tokens activations., and show the distribution of positive activations.  \n",
        "2. Logit weight distribution. This is the projection of the decoder weight onto the unembed and roughly gives us a sense of the tokens promoted by a feature. It's less useful in big models / middle layers.\n",
        "3. The top 10 and bottom 10 tokens in the logit weight distribution. \n",
        "4. **Max Activating Examples**. These are examples of text where the feature fires and usually provide the most information for helping us work out what a feature means. \n",
        "\n",
        "*Neuronpedia* is a website that hosts feature dashboards and which runs servers that can run the model and check feature activations. This makes it very convenient to check that a feature fires on the distribution of text you actually think it should fire on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Local: Finding Max Activating Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# instantiate an\n",
        "from sae_lens import ActivationsStore\n",
        "\n",
        "activation_store = ActivationsStore.from_sae(\n",
        "    model=model,\n",
        "    sae=sae,\n",
        "    streaming=True,\n",
        "    # fairly conservative parameters here so can use same for larger\n",
        "    # models without running out of memory.\n",
        "    store_batch_size_prompts=8,\n",
        "    train_batch_size_tokens=4096,\n",
        "    n_batches_in_buffer=32,\n",
        "    device=device,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def list_flatten(nested_list):\n",
        "    return [x for y in nested_list for x in y]\n",
        "\n",
        "def make_token_df(tokens, len_prefix=5, len_suffix=1, model = model):\n",
        "    str_tokens = [model.to_str_tokens(t) for t in tokens]\n",
        "    unique_token = [[f\"{s}/{i}\" for i, s in enumerate(str_tok)] for str_tok in str_tokens]\n",
        "    \n",
        "    context = []\n",
        "    prompt = []\n",
        "    pos = []\n",
        "    label = []\n",
        "    for b in range(tokens.shape[0]):\n",
        "        for p in range(tokens.shape[1]):\n",
        "            prefix = \"\".join(str_tokens[b][max(0, p-len_prefix):p])\n",
        "            if p==tokens.shape[1]-1:\n",
        "                suffix = \"\"\n",
        "            else:\n",
        "                suffix = \"\".join(str_tokens[b][p+1:min(tokens.shape[1]-1, p+1+len_suffix)])\n",
        "            current = str_tokens[b][p]\n",
        "            context.append(f\"{prefix}|{current}|{suffix}\")\n",
        "            prompt.append(b)\n",
        "            pos.append(p)\n",
        "            label.append(f\"{b}/{p}\")\n",
        "    # print(len(batch), len(pos), len(context), len(label))\n",
        "    return pd.DataFrame(dict(\n",
        "        str_tokens=list_flatten(str_tokens),\n",
        "        unique_token=list_flatten(unique_token),\n",
        "        context=context,\n",
        "        prompt=prompt,\n",
        "        pos=pos,\n",
        "        label=label,\n",
        "    ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# finding max activating examples is a bit harder. To do this we need to calculate feautre\n",
        "feature_list = torch.randint(0, sae.cfg.d_sae, (100,))\n",
        "examples_found = 0\n",
        "all_fired_tokens = []\n",
        "all_feature_acts = []\n",
        "all_reconstructions = []\n",
        "all_token_dfs = []\n",
        "\n",
        "total_batches = 100\n",
        "batch_size_prompts = activation_store.store_batch_size_prompts\n",
        "batch_size_tokens = activation_store.context_size * batch_size_prompts\n",
        "pbar = tqdm(range(total_batches))\n",
        "for i in pbar:\n",
        "    tokens = activation_store.get_batch_tokens()\n",
        "    tokens_df = make_token_df(tokens)\n",
        "    tokens_df[\"batch\"] = i\n",
        "    \n",
        "    flat_tokens = tokens.flatten()\n",
        "    \n",
        "    _, cache = model.run_with_cache(tokens, stop_at_layer = sae.cfg.hook_layer + 1, names_filter = [sae.cfg.hook_name])\n",
        "    sae_in = cache[sae.cfg.hook_name]\n",
        "    feature_acts = sae.encode(sae_in).squeeze()\n",
        "\n",
        "    feature_acts = feature_acts.flatten(0,1)\n",
        "    fired_mask = (feature_acts[:, feature_list]).sum(dim=-1) > 0\n",
        "    fired_tokens = model.to_str_tokens(flat_tokens[fired_mask])\n",
        "    reconstruction = feature_acts[fired_mask][:, feature_list] @ sae.W_dec[feature_list]\n",
        "\n",
        "    token_df = tokens_df.iloc[fired_mask.cpu().nonzero().flatten().numpy()]\n",
        "    all_token_dfs.append(token_df)\n",
        "    all_feature_acts.append(feature_acts[fired_mask][:, feature_list])\n",
        "    all_fired_tokens.append(fired_tokens)\n",
        "    all_reconstructions.append(reconstruction)\n",
        "    \n",
        "    examples_found += len(fired_tokens)\n",
        "    # print(f\"Examples found: {examples_found}\")\n",
        "    # update description\n",
        "    pbar.set_description(f\"Examples found: {examples_found}\")\n",
        "    \n",
        "# flatten the list of lists\n",
        "all_token_dfs = pd.concat(all_token_dfs)\n",
        "all_fired_tokens = list_flatten(all_fired_tokens)\n",
        "all_reconstructions = torch.cat(all_reconstructions)\n",
        "all_feature_acts = torch.cat(all_feature_acts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Getting Feature Activation Histogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_acts_df = pd.DataFrame(all_feature_acts.cpu().numpy(), columns = [f\"feature_{i}\" for i in feature_list])\n",
        "# all_token_dfs = pd.merge(all_token_dfs, feature_acts_df, left_index=True, right_index=True, suffixes=(\"\", \"\"))\n",
        "# all_token_dfs\n",
        "feature_acts_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_idx = 0\n",
        "# get non-zero activations\n",
        "\n",
        "all_positive_acts = all_feature_acts[all_feature_acts[:, feature_idx] > 0][:, feature_idx]\n",
        "prop_positive_activations = 100*len(all_positive_acts) / (total_batches*batch_size_tokens)\n",
        "\n",
        "px.histogram(\n",
        "    all_positive_acts.cpu(),\n",
        "    nbins=50,\n",
        "    title=f\"Histogram of positive activations - {prop_positive_activations:.3f}% of activations were positive\",\n",
        "    labels={\"value\": \"Activation\"},\n",
        "    width=800,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "top_10_activations = feature_acts_df.sort_values(f\"feature_{feature_list[0]}\", ascending=False).head(10)\n",
        "all_token_dfs.iloc[top_10_activations.index]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Getting the Top 10 Logit Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Shape of the decoder weights {sae.W_dec.shape})\")\n",
        "print(f\"Shape of the model unembed {model.W_U.shape}\")\n",
        "projection_matrix = sae.W_dec @ model.W_U\n",
        "print(f\"Shape of the projection matrix {projection_matrix.shape}\")\n",
        "\n",
        "# then we take the top_k tokens per feature and decode them\n",
        "top_k = 10\n",
        "# let's do this for 100 random features\n",
        "_, top_k_tokens = torch.topk(projection_matrix[feature_list], top_k, dim=1)\n",
        "\n",
        "\n",
        "feature_df = pd.DataFrame(top_k_tokens.cpu().numpy(), index = [f\"feature_{i}\" for i in random_features]).T\n",
        "feature_df.index = [f\"token_{i}\" for i in range(top_k)]\n",
        "feature_df.applymap(lambda x: model.tokenizer.decode(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Putting it all together: Compare against the Neuronpedia Dashboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "html = get_html(sae_release = \"gpt2-small\", sae_id=f\"{sae.cfg.hook_layer}-res-jb\", feature_idx=feature_list[0])\n",
        "IFrame(html, width=1200, height=600)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Advanced: Co-occurence Networks and Irriducible Subspaces\n",
        "\n",
        "Since we just wrote code very similar to the code we need for reproducing some of the analysis from \"Not All Language Models are Linear\", we show below how to regenerate their awesome circular representation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# only valid for res-jb resid_pre 7. \n",
        "day_of_the_week_features = [2592, 4445, 4663, 4733, 6531, 8179, 9566, 20927, 24185]\n",
        "# months_of_the_year = [3977, 4140, 5993, 7299, 9104, 9401, 10449, 11196, 12661, 14715, 17068, 17528, 19589, 21033, 22043, 23304]\n",
        "# years_of_10th_century = [1052, 2753, 4427, 6382, 8314, 9576, 9606, 13551, 19734, 20349]\n",
        "\n",
        "feature_list = day_of_the_week_features\n",
        "\n",
        "examples_found = 0\n",
        "all_fired_tokens = []\n",
        "all_feature_acts = []\n",
        "all_reconstructions = []\n",
        "all_token_dfs = []\n",
        "\n",
        "total_batches = 100\n",
        "batch_size_prompts = activation_store.store_batch_size_prompts\n",
        "batch_size_tokens = activation_store.context_size * batch_size_prompts\n",
        "pbar = tqdm(range(total_batches))\n",
        "for i in pbar:\n",
        "    tokens = activation_store.get_batch_tokens()\n",
        "    tokens_df = make_token_df(tokens)\n",
        "    tokens_df[\"batch\"] = i\n",
        "    \n",
        "    flat_tokens = tokens.flatten()\n",
        "    \n",
        "    _, cache = model.run_with_cache(tokens, stop_at_layer = sae.cfg.hook_layer + 1, names_filter = [sae.cfg.hook_name])\n",
        "    sae_in = cache[sae.cfg.hook_name]\n",
        "    feature_acts = sae.encode(sae_in).squeeze()\n",
        "\n",
        "    feature_acts = feature_acts.flatten(0,1)\n",
        "    fired_mask = (feature_acts[:, feature_list]).sum(dim=-1) > 0\n",
        "    fired_tokens = model.to_str_tokens(flat_tokens[fired_mask])\n",
        "    reconstruction = feature_acts[fired_mask][:, feature_list] @ sae.W_dec[feature_list]\n",
        "\n",
        "    token_df = tokens_df.iloc[fired_mask.cpu().nonzero().flatten().numpy()]\n",
        "    all_token_dfs.append(token_df)\n",
        "    all_feature_acts.append(feature_acts[fired_mask][:, feature_list])\n",
        "    all_fired_tokens.append(fired_tokens)\n",
        "    all_reconstructions.append(reconstruction)\n",
        "    \n",
        "    examples_found += len(fired_tokens)\n",
        "    # print(f\"Examples found: {examples_found}\")\n",
        "    # update description\n",
        "    pbar.set_description(f\"Examples found: {examples_found}\")\n",
        "    \n",
        "# flatten the list of lists\n",
        "all_token_dfs = pd.concat(all_token_dfs)\n",
        "all_fired_tokens = list_flatten(all_fired_tokens)\n",
        "all_reconstructions = torch.cat(all_reconstructions)\n",
        "all_feature_acts = torch.cat(all_feature_acts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# do PCA on reconstructions\n",
        "from sklearn.decomposition import PCA\n",
        "import plotly.express as px \n",
        "\n",
        "pca = PCA(n_components=3)\n",
        "pca_embedding = pca.fit_transform(all_reconstructions.cpu().numpy())\n",
        "\n",
        "pca_df = pd.DataFrame(pca_embedding, columns=[\"PC1\", \"PC2\", \"PC3\"])\n",
        "pca_df[\"tokens\"] = all_fired_tokens\n",
        "pca_df[\"context\"] = all_token_dfs.context.values\n",
        "\n",
        "\n",
        "px.scatter(\n",
        "    pca_df, x=\"PC2\", y=\"PC3\",\n",
        "    hover_data=[\"context\"],\n",
        "    hover_name=\"tokens\",\n",
        "    height = 800,\n",
        "    width = 1200,\n",
        "    color = \"tokens\",\n",
        "    title = \"PCA Subspace Reconstructions\",\n",
        ").show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Basics: Intervening on SAE Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Ablation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Steering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import numpy as np\n",
        "\n",
        "url = \"https://www.neuronpedia.org/api/steer\"\n",
        "\n",
        "payload = {\n",
        "    \"prompt\": \"A knight in shining\",\n",
        "    # \"prompt\": \"He had to fight back in self-\", \n",
        "    # \"prompt\": \"In the middle of the universe is the galactic\",\n",
        "    # \"prompt\": \"In the middle of the universe is the galactic\",\n",
        "    # \"prompt\": \"Oh no. We're running on empty. Its time to fill up the car with\",\n",
        "    # \"prompt\": \"Sure, I'm happy to pay. I don't have any cash on me but let me write you a\",\n",
        "    \"modelId\": \"gpt2-small\",\n",
        "    \"features\": [\n",
        "        {\n",
        "            \"modelId\": \"gpt2-small\",\n",
        "            \"layer\": \"7-res-jb\",\n",
        "            \"index\": 6770,\n",
        "            \"strength\": 8\n",
        "        }\n",
        "    ],\n",
        "    \"temperature\": 0.2,\n",
        "    \"n_tokens\": 2,\n",
        "    \"freq_penalty\": 1,\n",
        "    \"seed\": np.random.randint(100),\n",
        "    \"strength_multiplier\": 4\n",
        "}\n",
        "headers = {\"Content-Type\": \"application/json\"}\n",
        "\n",
        "response = requests.post(url, json=payload, headers=headers)\n",
        "\n",
        "print(response.json())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import numpy as np\n",
        "url = \"https://www.neuronpedia.org/api/steer\"\n",
        "\n",
        "payload = {\n",
        "    \"prompt\": \"I wrote a letter to my girlfiend. It said \\\"\",\n",
        "    \"modelId\": \"gpt2-small\",\n",
        "    \"features\": [\n",
        "        {\n",
        "            \"modelId\": \"gpt2-small\",\n",
        "            \"layer\": \"7-res-jb\",\n",
        "            \"index\": 20115,\n",
        "            \"strength\": 4\n",
        "        }\n",
        "    ],\n",
        "    \"temperature\": 0.7,\n",
        "    \"n_tokens\": 120,\n",
        "    \"freq_penalty\": 1,\n",
        "    \"seed\": np.random.randint(100),\n",
        "    \"strength_multiplier\": 4\n",
        "}\n",
        "headers = {\"Content-Type\": \"application/json\"}\n",
        "\n",
        "response = requests.post(url, json=payload, headers=headers)\n",
        "\n",
        "print(response.json())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Attribution\n",
        "\n",
        "Let's switch to an earlier SAE layer for attribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
        "    release = \"gpt2-small-res-jb\", # see other options in sae_lens/pretrained_saes.yaml\n",
        "    sae_id = \"blocks.7.hook_resid_pre\", # won't always be a hook point\n",
        "    device = device\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from functools import partial\n",
        "from typing import Any, Literal, NamedTuple, Callable\n",
        "\n",
        "import torch\n",
        "from sae_lens import SAE\n",
        "from transformer_lens import HookedTransformer\n",
        "from transformer_lens.hook_points import HookPoint\n",
        "\n",
        "\n",
        "class SaeReconstructionCache(NamedTuple):\n",
        "    sae_in: torch.Tensor\n",
        "    feature_acts: torch.Tensor\n",
        "    sae_out: torch.Tensor\n",
        "    sae_error: torch.Tensor\n",
        "\n",
        "\n",
        "def track_grad(tensor: torch.Tensor) -> None:\n",
        "    \"\"\"wrapper around requires_grad and retain_grad\"\"\"\n",
        "    tensor.requires_grad_(True)\n",
        "    tensor.retain_grad()\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ApplySaesAndRunOutput:\n",
        "    model_output: torch.Tensor\n",
        "    model_activations: dict[str, torch.Tensor]\n",
        "    sae_activations: dict[str, SaeReconstructionCache]\n",
        "\n",
        "    def zero_grad(self) -> None:\n",
        "        \"\"\"Helper to zero grad all tensors in this object.\"\"\"\n",
        "        self.model_output.grad = None\n",
        "        for act in self.model_activations.values():\n",
        "            act.grad = None\n",
        "        for cache in self.sae_activations.values():\n",
        "            cache.sae_in.grad = None\n",
        "            cache.feature_acts.grad = None\n",
        "            cache.sae_out.grad = None\n",
        "            cache.sae_error.grad = None\n",
        "\n",
        "\n",
        "def apply_saes_and_run(\n",
        "    model: HookedTransformer,\n",
        "    saes: dict[str, SAE],\n",
        "    input: Any,\n",
        "    include_error_term: bool = True,\n",
        "    track_model_hooks: list[str] | None = None,\n",
        "    return_type: Literal[\"logits\", \"loss\"] = \"logits\",\n",
        "    track_grads: bool = False,\n",
        ") -> ApplySaesAndRunOutput:\n",
        "    \"\"\"\n",
        "    Apply the SAEs to the model at the specific hook points, and run the model.\n",
        "    By default, this will include a SAE error term which guarantees that the SAE\n",
        "    will not affect model output. This function is designed to work correctly with\n",
        "    backprop as well, so it can be used for gradient-based feature attribution.\n",
        "\n",
        "    Args:\n",
        "        model: the model to run\n",
        "        saes: the SAEs to apply\n",
        "        input: the input to the model\n",
        "        include_error_term: whether to include the SAE error term to ensure the SAE doesn't affect model output. Default True\n",
        "        track_model_hooks: a list of hook points to record the activations and gradients. Default None\n",
        "        return_type: this is passed to the model.run_with_hooks function. Default \"logits\"\n",
        "        track_grads: whether to track gradients. Default False\n",
        "    \"\"\"\n",
        "\n",
        "    fwd_hooks = []\n",
        "    bwd_hooks = []\n",
        "\n",
        "    sae_activations: dict[str, SaeReconstructionCache] = {}\n",
        "    model_activations: dict[str, torch.Tensor] = {}\n",
        "\n",
        "    # this hook just track the SAE input, output, features, and error. If `track_grads=True`, it also ensures\n",
        "    # that requires_grad is set to True and retain_grad is called for intermediate values.\n",
        "    def reconstruction_hook(sae_in: torch.Tensor, hook: HookPoint, hook_point: str):  # noqa: ARG001\n",
        "        sae = saes[hook_point]\n",
        "        feature_acts = sae.encode(sae_in)\n",
        "        sae_out = sae.decode(feature_acts)\n",
        "        sae_error = (sae_in - sae_out).detach().clone()\n",
        "        if track_grads:\n",
        "            track_grad(sae_error)\n",
        "            track_grad(sae_out)\n",
        "            track_grad(feature_acts)\n",
        "            track_grad(sae_in)\n",
        "        sae_activations[hook_point] = SaeReconstructionCache(\n",
        "            sae_in=sae_in,\n",
        "            feature_acts=feature_acts,\n",
        "            sae_out=sae_out,\n",
        "            sae_error=sae_error,\n",
        "        )\n",
        "\n",
        "        if include_error_term:\n",
        "            return sae_out + sae_error\n",
        "        return sae_out\n",
        "\n",
        "    def sae_bwd_hook(output_grads: torch.Tensor, hook: HookPoint):  # noqa: ARG001\n",
        "        # this just passes the output grads to the input, so the SAE gets the same grads despite the error term hackery\n",
        "        return (output_grads,)\n",
        "\n",
        "    # this hook just records model activations, and ensures that intermediate activations have gradient tracking turned on if needed\n",
        "    def tracking_hook(hook_input: torch.Tensor, hook: HookPoint, hook_point: str):  # noqa: ARG001\n",
        "        model_activations[hook_point] = hook_input\n",
        "        if track_grads:\n",
        "            track_grad(hook_input)\n",
        "        return hook_input\n",
        "\n",
        "    for hook_point in saes.keys():\n",
        "        fwd_hooks.append(\n",
        "            (hook_point, partial(reconstruction_hook, hook_point=hook_point))\n",
        "        )\n",
        "        bwd_hooks.append((hook_point, sae_bwd_hook))\n",
        "    for hook_point in track_model_hooks or []:\n",
        "        fwd_hooks.append((hook_point, partial(tracking_hook, hook_point=hook_point)))\n",
        "\n",
        "    # now, just run the model while applying the hooks\n",
        "    with model.hooks(fwd_hooks=fwd_hooks, bwd_hooks=bwd_hooks):\n",
        "        model_output = model(input, return_type=return_type)\n",
        "\n",
        "    return ApplySaesAndRunOutput(\n",
        "        model_output=model_output,\n",
        "        model_activations=model_activations,\n",
        "        sae_activations=sae_activations,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from transformer_lens.hook_points import HookPoint\n",
        "from dataclasses import dataclass\n",
        "from functools import partial\n",
        "from typing import Any, Literal, NamedTuple\n",
        "\n",
        "import torch\n",
        "from sae_lens import SAE\n",
        "from transformer_lens import HookedTransformer\n",
        "from transformer_lens.hook_points import HookPoint\n",
        "\n",
        "EPS = 1e-8\n",
        "\n",
        "torch.set_grad_enabled(True)\n",
        "@dataclass\n",
        "class AttributionGrads:\n",
        "    metric: torch.Tensor\n",
        "    model_output: torch.Tensor\n",
        "    model_activations: dict[str, torch.Tensor]\n",
        "    sae_activations: dict[str, SaeReconstructionCache]\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Attribution:\n",
        "    model_attributions: dict[str, torch.Tensor]\n",
        "    model_activations: dict[str, torch.Tensor]\n",
        "    model_grads: dict[str, torch.Tensor]\n",
        "    sae_feature_attributions: dict[str, torch.Tensor]\n",
        "    sae_feature_activations: dict[str, torch.Tensor]\n",
        "    sae_feature_grads: dict[str, torch.Tensor]\n",
        "    sae_errors_attribution_proportion: dict[str, float]\n",
        "\n",
        "\n",
        "def calculate_attribution_grads(\n",
        "    model: HookedSAETransformer,\n",
        "    prompt: str,\n",
        "    metric_fn: Callable[[torch.Tensor], torch.Tensor],\n",
        "    track_hook_points: list[str] | None = None,\n",
        "    include_saes: dict[str, SAE] | None = None,\n",
        "    return_logits: bool = True,\n",
        "    include_error_term: bool = True,\n",
        ") -> AttributionGrads:\n",
        "    \"\"\"\n",
        "    Wrapper around apply_saes_and_run that calculates gradients wrt to the metric_fn.\n",
        "    Tracks grads for both SAE feature and model neurons, and returns them in a structured format.\n",
        "    \"\"\"\n",
        "    output = apply_saes_and_run(\n",
        "        model,\n",
        "        saes=include_saes or {},\n",
        "        input=prompt,\n",
        "        return_type=\"logits\" if return_logits else \"loss\",\n",
        "        track_model_hooks=track_hook_points,\n",
        "        include_error_term=include_error_term,\n",
        "        track_grads=True,\n",
        "    )\n",
        "    metric = metric_fn(output.model_output)\n",
        "    output.zero_grad()\n",
        "    metric.backward()\n",
        "    return AttributionGrads(\n",
        "        metric=metric,\n",
        "        model_output=output.model_output,\n",
        "        model_activations=output.model_activations,\n",
        "        sae_activations=output.sae_activations,\n",
        "    )\n",
        "\n",
        "\n",
        "def calculate_feature_attribution(\n",
        "    model: HookedSAETransformer,\n",
        "    input: Any,\n",
        "    metric_fn: Callable[[torch.Tensor], torch.Tensor],\n",
        "    track_hook_points: list[str] | None = None,\n",
        "    include_saes: dict[str, SAE] | None = None,\n",
        "    return_logits: bool = True,\n",
        "    include_error_term: bool = True,\n",
        ") -> Attribution:\n",
        "    \"\"\"\n",
        "    Calculate feature attribution for SAE features and model neurons following\n",
        "    the procedure in https://transformer-circuits.pub/2024/march-update/index.html#feature-heads.\n",
        "    This include the SAE error term by default, so inserting the SAE into the calculation is\n",
        "    guaranteed to not affect the model output. This can be disabled by setting `include_error_term=False`.\n",
        "\n",
        "    Args:\n",
        "        model: The model to calculate feature attribution for.\n",
        "        input: The input to the model.\n",
        "        metric_fn: A function that takes the model output and returns a scalar metric.\n",
        "        track_hook_points: A list of model hook points to track activations for, if desired\n",
        "        include_saes: A dictionary of SAEs to include in the calculation. The key is the hook point to apply the SAE to.\n",
        "        return_logits: Whether to return the model logits or loss. This is passed to TLens, so should match whatever the metric_fn expects (probably logits)\n",
        "        include_error_term: Whether to include the SAE error term in the calculation. This is recommended, as it ensures that the SAE will not affecting the model output.\n",
        "    \"\"\"\n",
        "    # first, calculate gradients wrt to the metric_fn.\n",
        "    # these will be multiplied with the activation values to get the attributions\n",
        "    outputs_with_grads = calculate_attribution_grads(\n",
        "        model,\n",
        "        input,\n",
        "        metric_fn,\n",
        "        track_hook_points,\n",
        "        include_saes=include_saes,\n",
        "        return_logits=return_logits,\n",
        "        include_error_term=include_error_term,\n",
        "    )\n",
        "    model_attributions = {}\n",
        "    model_activations = {}\n",
        "    model_grads = {}\n",
        "    sae_feature_attributions = {}\n",
        "    sae_feature_activations = {}\n",
        "    sae_feature_grads = {}\n",
        "    sae_error_proportions = {}\n",
        "    # this code is long, but all it's doing is multiplying the grads by the activations\n",
        "    # and recording grads, acts, and attributions in dictionaries to return to the user\n",
        "    with torch.no_grad():\n",
        "        for name, act in outputs_with_grads.model_activations.items():\n",
        "            assert act.grad is not None\n",
        "            raw_activation = act.detach().clone()\n",
        "            model_attributions[name] = (act.grad * raw_activation).detach().clone()\n",
        "            model_activations[name] = raw_activation\n",
        "            model_grads[name] = act.grad.detach().clone()\n",
        "        for name, act in outputs_with_grads.sae_activations.items():\n",
        "            assert act.feature_acts.grad is not None\n",
        "            assert act.sae_out.grad is not None\n",
        "            raw_activation = act.feature_acts.detach().clone()\n",
        "            sae_feature_attributions[name] = (\n",
        "                (act.feature_acts.grad * raw_activation).detach().clone()\n",
        "            )\n",
        "            sae_feature_activations[name] = raw_activation\n",
        "            sae_feature_grads[name] = act.feature_acts.grad.detach().clone()\n",
        "            if include_error_term:\n",
        "                assert act.sae_error.grad is not None\n",
        "                error_grad_norm = act.sae_error.grad.norm().item()\n",
        "            else:\n",
        "                error_grad_norm = 0\n",
        "            sae_out_norm = act.sae_out.grad.norm().item()\n",
        "            sae_error_proportions[name] = error_grad_norm / (\n",
        "                sae_out_norm + error_grad_norm + EPS\n",
        "            )\n",
        "        return Attribution(\n",
        "            model_attributions=model_attributions,\n",
        "            model_activations=model_activations,\n",
        "            model_grads=model_grads,\n",
        "            sae_feature_attributions=sae_feature_attributions,\n",
        "            sae_feature_activations=sae_feature_activations,\n",
        "            sae_feature_grads=sae_feature_grads,\n",
        "            sae_errors_attribution_proportion=sae_error_proportions,\n",
        "        )\n",
        "        \n",
        "        \n",
        "# prompt = \" Tiger Woods plays the sport of\"\n",
        "# pos_token = model.tokenizer.encode(\" golf\")[0]\n",
        "prompt = \"In the beginning, God created the heavens and the\"\n",
        "pos_token = model.tokenizer.encode(\" earth\")\n",
        "neg_token = model.tokenizer.encode(\" sky\")\n",
        "def metric_fn(logits: torch.tensor, pos_token: torch.tensor =pos_token, neg_token: torch.Tensor=neg_token) -> torch.Tensor:\n",
        "    return logits[0,-1,pos_token] - logits[0,-1,neg_token]\n",
        "\n",
        "feature_attribution_df = calculate_feature_attribution(\n",
        "    input = prompt,\n",
        "    model = model,\n",
        "    metric_fn = metric_fn,\n",
        "    include_saes={sae.cfg.hook_name: sae},\n",
        "    include_error_term=True,\n",
        "    return_logits=True,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformer_lens.utils import test_prompt\n",
        "test_prompt(prompt, model.to_string(pos_token), model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokens = model.to_str_tokens(prompt)\n",
        "unique_tokens = [f\"{i}/{t}\" for i, t in enumerate(tokens)]\n",
        "\n",
        "px.bar(x = unique_tokens,\n",
        "       y = feature_attribution_df.sae_feature_attributions[sae.cfg.hook_name][0].sum(-1).detach().cpu().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_sparse_feature_to_long_df(sparse_tensor: torch.Tensor) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Convert a sparse tensor to a long format pandas DataFrame.\n",
        "    \"\"\"\n",
        "    df = pd.DataFrame(sparse_tensor.detach().cpu().numpy())\n",
        "    df_long = df.melt(ignore_index=False, var_name='column', value_name='value')\n",
        "    df_long.columns = [\"feature\", \"attribution\"]\n",
        "    df_long_nonzero = df_long[df_long['attribution'] != 0]\n",
        "    df_long_nonzero = df_long_nonzero.reset_index().rename(columns={'index': 'position'})\n",
        "    return df_long_nonzero\n",
        "\n",
        "df_long_nonzero = convert_sparse_feature_to_long_df(feature_attribution_df.sae_feature_attributions[sae.cfg.hook_name][0])\n",
        "df_long_nonzero.sort_values(\"attribution\", ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i, v in df_long_nonzero.query(\"position==8\").groupby(\"feature\").attribution.sum().sort_values(ascending=False).head(5).items():\n",
        "    print(f\"Feature {i} had a total attribution of {v:.2f}\")\n",
        "    html = get_html(sae_release = \"gpt2-small\", sae_id=f\"{sae.cfg.hook_layer}-res-jb\", feature_idx=int(i))\n",
        "    display(IFrame(html, width=1200, height=300))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i, v in df_long_nonzero.groupby(\"feature\").attribution.sum().sort_values(ascending=False).head(5).items():\n",
        "    print(f\"Feature {i} had a total attribution of {v:.2f}\")\n",
        "    html = get_html(sae_release = \"gpt2-small\", sae_id=f\"{sae.cfg.hook_layer}-res-jb\", feature_idx=int(i))\n",
        "    display(IFrame(html, width=1200, height=300))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Advanced: Making U-Maps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
