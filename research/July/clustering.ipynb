{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dev Code for U-Maps for any given SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "import hdbscan\n",
    "import pandas as pd \n",
    "import torch \n",
    "from sae_lens import SAE\n",
    "from transformer_lens import HookedTransformer\n",
    "torch.set_grad_enabled(False)\n",
    "import plotly.express as px\n",
    "import os \n",
    "from tqdm import tqdm \n",
    "import gc\n",
    "import json\n",
    "\n",
    "\n",
    "def get_neuronpedia_umap_and_clusters(\n",
    "    release_id: str = \"res-jb\",\n",
    "    sae_id: str = \"blocks.11.hook_resid_post\",\n",
    "    n_neighbors_visual: int = 15,\n",
    "    min_dist_visual: float = 0.05,\n",
    "    n_neighbors_cluster: float = 15,\n",
    "    min_dist_cluster: float = 0.1,\n",
    "    min_cluster_size: int = 3,\n",
    "    plot: bool = True\n",
    "):\n",
    "    '''\n",
    "    This function will generate UMAP and cluster plots for the SAE specified by the release_id and sae_id.\n",
    "    The plots will be saved in the output folder.\n",
    "    \n",
    "    Args:\n",
    "    release_id: str\n",
    "        The release id of the SAE to be visualized.\n",
    "    sae_id: str\n",
    "        The sae_id of the SAE to be visualized.\n",
    "    n_neighbors_visual: int\n",
    "        The number of neighbors to consider for the UMAP embedding for the visual plot.\n",
    "    min_dist_visual: float\n",
    "        The minimum distance between points in the UMAP embedding for the visual plot.\n",
    "    n_neighbors_cluster: int    \n",
    "        The number of neighbors to consider for the UMAP embedding for the cluster plot.\n",
    "    min_dist_cluster: float\n",
    "        The minimum distance between points in the UMAP embedding for the cluster plot.\n",
    "    min_cluster_size: int\n",
    "        The minimum number of points in a cluster.\n",
    "    plot: bool\n",
    "        Whether to plot the UMAP embeddings or not.\n",
    "    '''\n",
    "    # make output folder:\n",
    "    os.makedirs(f\"output/{release_id}/{sae_id}\", exist_ok=True)\n",
    "    \n",
    "    sae, _, sparsity = SAE.from_pretrained(\n",
    "        release = release_id,\n",
    "        sae_id = sae_id,\n",
    "        device = \"mps\",\n",
    "    )\n",
    "    sae.fold_W_dec_norm()\n",
    "    embedding = sae.W_dec.cpu()\n",
    "\n",
    "    if sparsity is None: \n",
    "        sparsity = torch.zeros_like(embedding[:,0])\n",
    "        \n",
    "    feature_df = pd.DataFrame(sparsity.cpu(), index = [f\"feature_{i}\" for i in range(embedding.shape[0])], columns=[\"sparsity\"])\n",
    "    \n",
    "    if plot:\n",
    "        # Assume in t-lens for now\n",
    "        model = HookedTransformer.from_pretrained_no_processing(sae.cfg.model_name, fold_ln=True)\n",
    "        W_U = model.W_U\n",
    "        tokenizer = model.tokenizer\n",
    "        del model \n",
    "        gc.collect()\n",
    "\n",
    "        if not os.path.exists(f\"output/{release_id}/{sae_id}/feature_df.csv\"):\n",
    "\n",
    "            # Get total number of rows in W_dec\n",
    "            total_rows = sae.W_dec.shape[0]\n",
    "\n",
    "            results = []\n",
    "\n",
    "            batch_size = sae.W_dec.shape[0] // 4\n",
    "            # Process in batches\n",
    "            for i in range(0, total_rows, batch_size):\n",
    "                # Calculate end index for current batch\n",
    "                end_idx = min(i + batch_size, total_rows)\n",
    "                \n",
    "                # Process a batch\n",
    "                batch_result = sae.W_dec[i:end_idx] @ W_U\n",
    "                \n",
    "                # Get top k values and indices for the batch\n",
    "                batch_vals, batch_indices = torch.topk(batch_result, 10)\n",
    "                \n",
    "                # Store results\n",
    "                results.append((batch_vals, batch_indices))\n",
    "\n",
    "            # Combine results\n",
    "            token_factors_inds = torch.cat([r[1] for r in results])\n",
    "\n",
    "            feature_df[\"tok_token_ids\"] = token_factors_inds.tolist()\n",
    "            feature_df[\"top_token_strs\"] = feature_df[\"tok_token_ids\"].apply(lambda x: [tokenizer.decode([i]) for i in x]) # type: ignore\n",
    "            feature_df[\"top_token_strs_formatted\"] = feature_df[\"top_token_strs\"].apply(lambda x: \",\".join(x))\n",
    "        else:\n",
    "            feature_df = pd.read_csv(f\"output/{release_id}/{sae_id}/feature_df.csv\", index_col=0)\n",
    "            feature_df[\"tok_token_ids\"] = feature_df[\"tok_token_ids\"].apply(lambda x: eval(x))\n",
    "            feature_df[\"top_token_strs\"] = feature_df[\"top_token_strs\"].apply(lambda x: eval(x))\n",
    "            feature_df[\"top_token_strs_formatted\"] = feature_df[\"top_token_strs\"].apply(lambda x: \",\".join(x))\n",
    "\n",
    "\n",
    "    # 2. Visual UMAP\n",
    "    print(\"Calculating 2D UMAP\")\n",
    "    visual_umap = UMAP(n_components=2, n_neighbors=n_neighbors_visual, min_dist=min_dist_visual, metric='cosine')\n",
    "    embedding = sae.W_dec.cpu()\n",
    "    visual_umap_embedding = visual_umap.fit_transform(embedding)\n",
    "\n",
    "    feature_df[\"umap_x\"] = visual_umap_embedding[:,0] # type: ignore\n",
    "    feature_df[\"umap_y\"] = visual_umap_embedding[:,1] # type: ignore\n",
    "\n",
    "    # 3: Cluster UMAP\n",
    "    print(\"Calculating 10D UMAP\")\n",
    "    clustering_umap = UMAP(n_components=10, n_neighbors=n_neighbors_cluster, min_dist=min_dist_cluster, metric='cosine')\n",
    "    clustering_umap_embedding = clustering_umap.fit_transform(embedding)\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size)\n",
    "    clusterer.fit(clustering_umap_embedding)\n",
    "\n",
    "    feature_df[\"cluster\"] = clusterer.labels_\n",
    "    feature_df.sort_values(\"cluster\", inplace=True)\n",
    "    feature_df[\"cluster\"] = feature_df[\"cluster\"].astype(str)\n",
    "\n",
    "    if plot:\n",
    "        print(\"Plotting\")\n",
    "        fig = px.scatter(\n",
    "            feature_df,\n",
    "            x=\"umap_x\",\n",
    "            y=\"umap_y\",\n",
    "            color=\"cluster\",\n",
    "            height=1200,\n",
    "            width =1600,\n",
    "            hover_data= [\"top_token_strs_formatted\"] if \"top_token_strs_formatted\" in feature_df.columns else None,\n",
    "        )\n",
    "            \n",
    "        # reduce point size \n",
    "        fig.update_traces(marker=dict(size=2))\n",
    "        fig.write_html(f\"output/{release_id}/{sae_id}/umap_.html\")\n",
    "\n",
    "    print(\"Saving\")\n",
    "    feature_df.to_csv(f\"output/{release_id}/{sae_id}/feature_df.csv\")\n",
    "    \n",
    "    # save config as well (in json)\n",
    "    cfg = {\n",
    "        \"release_id\": release_id,\n",
    "        \"sae_id\": sae_id,\n",
    "        \"n_neighbors_visual\": n_neighbors_visual,\n",
    "        \"min_dist_visual\": min_dist_visual,\n",
    "        \"n_neighbors_cluster\": n_neighbors_cluster,\n",
    "        \"min_dist_cluster\": min_dist_cluster,\n",
    "        \"min_cluster_size\": min_cluster_size,\n",
    "    }\n",
    "    with open(f\"output/{release_id}/{sae_id}/umap_cfg.json\", \"w\") as f:\n",
    "        json.dump(cfg, f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running gpt2-small-hook-z-kk blocks.0.hook_z\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Calculating 2D UMAP\n",
      "Calculating 10D UMAP\n",
      "Plotting\n",
      "Saving\n",
      "Running gpt2-small-hook-z-kk blocks.1.hook_z\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Calculating 2D UMAP\n",
      "Calculating 10D UMAP\n",
      "Plotting\n",
      "Saving\n",
      "Running gpt2-small-hook-z-kk blocks.2.hook_z\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Calculating 2D UMAP\n",
      "Calculating 10D UMAP\n",
      "Plotting\n",
      "Saving\n",
      "Running gpt2-small-hook-z-kk blocks.3.hook_z\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Calculating 2D UMAP\n",
      "Calculating 10D UMAP\n",
      "Plotting\n",
      "Saving\n",
      "Running gpt2-small-hook-z-kk blocks.4.hook_z\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Calculating 2D UMAP\n",
      "Calculating 10D UMAP\n",
      "Plotting\n",
      "Saving\n",
      "Running gpt2-small-hook-z-kk blocks.5.hook_z\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Calculating 2D UMAP\n",
      "Calculating 10D UMAP\n",
      "Plotting\n",
      "Saving\n",
      "Running gpt2-small-hook-z-kk blocks.6.hook_z\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Calculating 2D UMAP\n",
      "Calculating 10D UMAP\n",
      "Plotting\n",
      "Saving\n",
      "Running gpt2-small-hook-z-kk blocks.7.hook_z\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Calculating 2D UMAP\n",
      "Calculating 10D UMAP\n",
      "Plotting\n",
      "Saving\n",
      "Running gpt2-small-hook-z-kk blocks.8.hook_z\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Calculating 2D UMAP\n",
      "Calculating 10D UMAP\n",
      "Plotting\n",
      "Saving\n",
      "Running gpt2-small-hook-z-kk blocks.9.hook_z\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Calculating 2D UMAP\n",
      "Calculating 10D UMAP\n",
      "Plotting\n",
      "Saving\n",
      "Running gpt2-small-hook-z-kk blocks.10.hook_z\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Calculating 2D UMAP\n",
      "Calculating 10D UMAP\n",
      "Plotting\n",
      "Saving\n",
      "Running gpt2-small-hook-z-kk blocks.11.hook_z\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Calculating 2D UMAP\n",
      "Calculating 10D UMAP\n",
      "Plotting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [04:42<18:50, 141.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving\n",
      "Running gpt2-small-mlp-tm blocks.0.hook_mlp_out\n",
      "Loaded pretrained model gpt2 into HookedTransformer\n",
      "Calculating 2D UMAP\n",
      "Calculating 10D UMAP\n",
      "Plotting\n",
      "Saving\n",
      "Running gpt2-small-mlp-tm blocks.1.hook_mlp_out\n",
      "Loaded pretrained model gpt2 into HookedTransformer\n",
      "Calculating 2D UMAP\n",
      "Calculating 10D UMAP\n",
      "Plotting\n",
      "Saving\n",
      "Running gpt2-small-mlp-tm blocks.2.hook_mlp_out\n",
      "Loaded pretrained model gpt2 into HookedTransformer\n",
      "Calculating 2D UMAP\n",
      "Calculating 10D UMAP\n",
      "Plotting\n",
      "Saving\n",
      "Running gpt2-small-mlp-tm blocks.3.hook_mlp_out\n",
      "Loaded pretrained model gpt2 into HookedTransformer\n",
      "Calculating 2D UMAP\n",
      "Calculating 10D UMAP\n",
      "Plotting\n",
      "Saving\n",
      "Running gpt2-small-mlp-tm blocks.4.hook_mlp_out\n",
      "Loaded pretrained model gpt2 into HookedTransformer\n",
      "Calculating 2D UMAP\n",
      "Calculating 10D UMAP\n",
      "Plotting\n",
      "Saving\n",
      "Running gpt2-small-mlp-tm blocks.5.hook_mlp_out\n",
      "Loaded pretrained model gpt2 into HookedTransformer\n",
      "Calculating 2D UMAP\n",
      "Calculating 10D UMAP\n",
      "Plotting\n",
      "Saving\n",
      "Running gpt2-small-mlp-tm blocks.6.hook_mlp_out\n",
      "Loaded pretrained model gpt2 into HookedTransformer\n",
      "Calculating 2D UMAP\n",
      "Calculating 10D UMAP\n",
      "Plotting\n",
      "Saving\n",
      "Running gpt2-small-mlp-tm blocks.7.hook_mlp_out\n",
      "Loaded pretrained model gpt2 into HookedTransformer\n",
      "Calculating 2D UMAP\n",
      "Calculating 10D UMAP\n",
      "Plotting\n",
      "Saving\n",
      "Running gpt2-small-mlp-tm blocks.8.hook_mlp_out\n",
      "Loaded pretrained model gpt2 into HookedTransformer\n",
      "Calculating 2D UMAP\n",
      "Calculating 10D UMAP\n",
      "Plotting\n",
      "Saving\n",
      "Running gpt2-small-mlp-tm blocks.9.hook_mlp_out\n",
      "Loaded pretrained model gpt2 into HookedTransformer\n",
      "Calculating 2D UMAP\n",
      "Calculating 10D UMAP\n",
      "Plotting\n",
      "Saving\n",
      "Running gpt2-small-mlp-tm blocks.10.hook_mlp_out\n",
      "Loaded pretrained model gpt2 into HookedTransformer\n",
      "Calculating 2D UMAP\n",
      "Calculating 10D UMAP\n",
      "Plotting\n",
      "Saving\n",
      "Running gpt2-small-mlp-tm blocks.11.hook_mlp_out\n",
      "Loaded pretrained model gpt2 into HookedTransformer\n",
      "Calculating 2D UMAP\n",
      "Calculating 10D UMAP\n",
      "Plotting\n",
      "Saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [09:05<22:24, 192.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running gpt2-small-res-jb-feature-splitting blocks.8.hook_resid_pre_768\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Calculating 2D UMAP\n",
      "Calculating 10D UMAP\n",
      "Plotting\n",
      "Saving\n",
      "Running gpt2-small-res-jb-feature-splitting blocks.8.hook_resid_pre_1536\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Calculating 2D UMAP\n",
      "Calculating 10D UMAP\n",
      "Plotting\n",
      "Saving\n",
      "Running gpt2-small-res-jb-feature-splitting blocks.8.hook_resid_pre_3072\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Calculating 2D UMAP\n",
      "Calculating 10D UMAP\n",
      "Plotting\n",
      "Saving\n",
      "Running gpt2-small-res-jb-feature-splitting blocks.8.hook_resid_pre_6144\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Calculating 2D UMAP\n",
      "Calculating 10D UMAP\n",
      "Plotting\n",
      "Saving\n",
      "Running gpt2-small-res-jb-feature-splitting blocks.8.hook_resid_pre_12288\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Calculating 2D UMAP\n",
      "Calculating 10D UMAP\n",
      "Plotting\n",
      "Saving\n",
      "Running gpt2-small-res-jb-feature-splitting blocks.8.hook_resid_pre_24576\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Calculating 2D UMAP\n",
      "Calculating 10D UMAP\n",
      "Plotting\n",
      "Saving\n",
      "Running gpt2-small-res-jb-feature-splitting blocks.8.hook_resid_pre_49152\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Calculating 2D UMAP\n",
      "Calculating 10D UMAP\n",
      "Plotting\n",
      "Saving\n",
      "Running gpt2-small-res-jb-feature-splitting blocks.8.hook_resid_pre_98304\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Calculating 2D UMAP\n",
      "Calculating 10D UMAP\n",
      "Plotting\n",
      "Saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [12:39<20:00, 200.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running gpt2-small-resid-post-v5-32k blocks.0.hook_resid_post\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Calculating 2D UMAP\n",
      "Calculating 10D UMAP\n",
      "Plotting\n",
      "Saving\n",
      "Running gpt2-small-resid-post-v5-32k blocks.1.hook_resid_post\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Calculating 2D UMAP\n",
      "Calculating 10D UMAP\n",
      "Plotting\n",
      "Saving\n",
      "Running gpt2-small-resid-post-v5-32k blocks.2.hook_resid_post\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Calculating 2D UMAP\n",
      "Calculating 10D UMAP\n",
      "Plotting\n",
      "Saving\n",
      "Running gpt2-small-resid-post-v5-32k blocks.3.hook_resid_post\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Calculating 2D UMAP\n",
      "Calculating 10D UMAP\n",
      "Plotting\n",
      "Saving\n",
      "Running gpt2-small-resid-post-v5-32k blocks.4.hook_resid_post\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Calculating 2D UMAP\n",
      "Calculating 10D UMAP\n",
      "Plotting\n",
      "Saving\n",
      "Running gpt2-small-resid-post-v5-32k blocks.5.hook_resid_post\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Calculating 2D UMAP\n",
      "Calculating 10D UMAP\n",
      "Plotting\n",
      "Saving\n",
      "Running gpt2-small-resid-post-v5-32k blocks.6.hook_resid_post\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Calculating 2D UMAP\n",
      "Calculating 10D UMAP\n",
      "Plotting\n",
      "Saving\n",
      "Running gpt2-small-resid-post-v5-32k blocks.7.hook_resid_post\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Calculating 2D UMAP\n",
      "Calculating 10D UMAP\n",
      "Plotting\n",
      "Saving\n",
      "Running gpt2-small-resid-post-v5-32k blocks.8.hook_resid_post\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Calculating 2D UMAP\n",
      "Calculating 10D UMAP\n",
      "Plotting\n",
      "Saving\n",
      "Running gpt2-small-resid-post-v5-32k blocks.9.hook_resid_post\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Calculating 2D UMAP\n",
      "Calculating 10D UMAP\n",
      "Plotting\n",
      "Saving\n",
      "Running gpt2-small-resid-post-v5-32k blocks.10.hook_resid_post\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Calculating 2D UMAP\n",
      "Calculating 10D UMAP\n",
      "Plotting\n",
      "Saving\n",
      "Running gpt2-small-resid-post-v5-32k blocks.11.hook_resid_post\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Calculating 2D UMAP\n",
      "Calculating 10D UMAP\n",
      "Plotting\n",
      "Saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [18:14<20:35, 247.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running gemma-2b-res-jb blocks.0.hook_resid_post\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0c2edc723204ac99794145310f2f82f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2b into HookedTransformer\n",
      "Calculating 2D UMAP\n",
      "Calculating 10D UMAP\n",
      "Plotting\n",
      "Saving\n",
      "Running gemma-2b-res-jb blocks.6.hook_resid_post\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26e3fb76dc5e45c2905fa3a31374fb0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2b into HookedTransformer\n",
      "Calculating 2D UMAP\n",
      "Calculating 10D UMAP\n",
      "Plotting\n",
      "Saving\n",
      "Running gemma-2b-res-jb blocks.12.hook_resid_post\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b601d20ada1a4190ada444e512ddc98b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2b into HookedTransformer\n",
      "Calculating 2D UMAP\n",
      "Calculating 10D UMAP\n",
      "Plotting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [23:52<10:24, 208.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving\n",
      "Running gemma-2b-it-res-jb blocks.12.hook_resid_post\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47cc8d89d73b4eee812906a24d41d11c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2b-it into HookedTransformer\n",
      "Calculating 2D UMAP\n",
      "Calculating 10D UMAP\n",
      "Plotting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [25:56<06:12, 186.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving\n",
      "Running mistral-7b-res-wg blocks.8.hook_resid_pre\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "485a77942d184634ae1304ee70c69065",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5043f6aac82e49da9af21bf30b8dc6f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "602f5885373c421eb6ace9b43872e66e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/996 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model mistral-7b into HookedTransformer\n",
      "Calculating 2D UMAP\n",
      "Calculating 10D UMAP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting\n",
      "Saving\n",
      "Running mistral-7b-res-wg blocks.16.hook_resid_pre\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4fba1b45b8a429d9d3a7e7bead132c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sae_weights.safetensors:   0%|          | 0.00/2.15G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3163f7c2d8404edbafa10614474a2eef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model mistral-7b into HookedTransformer\n",
      "Calculating 2D UMAP\n",
      "Calculating 10D UMAP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting\n",
      "Saving\n",
      "Running mistral-7b-res-wg blocks.24.hook_resid_pre\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e91b9b439069486987d09c59f7d43b62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sae_weights.safetensors:   0%|          | 0.00/2.15G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80af6cf1383c465f93e3feedb0fda9ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model mistral-7b into HookedTransformer\n",
      "Calculating 2D UMAP\n",
      "Calculating 10D UMAP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting\n",
      "Saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [48:12<00:00, 289.28s/it]\n",
      "100%|██████████| 10/10 [48:12<00:00, 289.28s/it]\n"
     ]
    }
   ],
   "source": [
    "from sae_lens.toolkit.pretrained_saes_directory import get_pretrained_saes_directory\n",
    "\n",
    "all_loadable_saes = []\n",
    "saes_directory = get_pretrained_saes_directory()\n",
    "for release, lookup in tqdm(saes_directory.items()):\n",
    "    if release not in [\"gpt2-small-res-jb\", \"sae-llama-3-8b-eai\",\"gpt2-small-resid-post-v5-128k\"]:\n",
    "        for sae_name in lookup.saes_map.keys():\n",
    "            print(f\"Running {release} {sae_name}\")\n",
    "            get_neuronpedia_umap_and_clusters(release, sae_name, plot=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
